# -*- coding: utf-8 -*-
"""ByteTrack_modifications.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hIpF_GNbxxthONdmhR_Crsp-aYuivBra
"""

# 1️⃣ Install required system packages for lap
!apt-get install -y liblapack-dev libblas-dev libopencv-dev pkg-config

# 2️⃣ Install lap from PyPI with prebuilt wheels
!pip install cython
!pip install lapx  # this is the maintained fork with wheels

# 3️⃣ Install YOLOv8
!pip install ultralytics

# 4️⃣ Install ByteTrack dependencies
!pip install onemetric
!pip install -U git+https://github.com/ifzhang/ByteTrack.git
!pip install roboflow
!pip install ultralytics
!pip install scikit-learn tqdm
!pip install -U pip
!pip install -U ultralytics
!pip install yolox loguru cython_bbox lap --quiet



from ultralytics import YOLO
from roboflow import Roboflow

import cv2
import numpy as np
from yolox.tracker.byte_tracker import BYTETracker
import numpy as np
np.float = float

rf = Roboflow(api_key="91TqmIgv8w7tUi53b8Kb")
project = rf.workspace("pipeline-detection").project("pipeline-detection-copy")
version = project.version(1)
dataset1 = version.download("yolov8")

rf = Roboflow(api_key="PA3kj1H2PlxAKb2sylWl")
project = rf.workspace("mariem-vgz4r").project("pipeline-9nae1")
version = project.version(1)
dataset2 = version.download("yolov8")

rf = Roboflow(api_key="KHlvL8wWXLMrMTQSGysw")
project = rf.workspace("projects-gzqta").project("pipes-aapxp")
version = project.version(2)
dataset3 = version.download("yolov8")

import os
import shutil
from glob import glob

# Paths to your datasets
dataset_paths = [
    dataset1.location,  # path from Roboflow download
    dataset2.location,
    dataset3.location
]

# Create merged dataset structure
merged_dataset_path = "/content/merged_dataset"
for split in ["train", "valid", "test"]:
    os.makedirs(os.path.join(merged_dataset_path, split, "images"), exist_ok=True)
    os.makedirs(os.path.join(merged_dataset_path, split, "labels"), exist_ok=True)

# Merge datasets
for dataset_path in dataset_paths:
    for split in ["train", "valid", "test"]:
        img_dir = os.path.join(dataset_path, split, "images")
        lbl_dir = os.path.join(dataset_path, split, "labels")

        if not os.path.exists(img_dir) or not os.path.exists(lbl_dir):
            print(f"⚠️ Missing {split} folder in {dataset_path}, skipping...")
            continue

        img_src = glob(os.path.join(img_dir, "*"))
        lbl_src = glob(os.path.join(lbl_dir, "*"))

        if not img_src:
            print(f"⚠️ No images found in {img_dir}")
        if not lbl_src:
            print(f"⚠️ No labels found in {lbl_dir}")

        for img_file in img_src:
            shutil.copy(img_file, os.path.join(merged_dataset_path, split, "images"))
        for lbl_file in lbl_src:
            shutil.copy(lbl_file, os.path.join(merged_dataset_path, split, "labels"))

        print(f"✅ {split}: copied {len(img_src)} images and {len(lbl_src)} labels from {dataset_path}")

print("🎯 Merge complete!")

# Final count check
for split in ["train", "valid", "test"]:
    img_count = len(glob(os.path.join(merged_dataset_path, split, "images", "*")))
    lbl_count = len(glob(os.path.join(merged_dataset_path, split, "labels", "*")))
    print(f"📂 {split}: {img_count} images, {lbl_count} labels")

from google.colab import drive
import shutil

# 1. Mount Google Drive
#drive.mount('/content/drive')

# 2. Define paths
#src_folder = "/content/merged_dataset"
#dst_folder = "/content/drive/MyDrive/Colab Notebooks/DataSet/Pipe1"

# 3. Copy the entire folder to Google Drive
#shutil.copytree(src_folder, dst_folder)
#print("✅ Folder copied to Google Drive at:", dst_folder)

!pip install pyyaml -q
import yaml

# Classes list from your datasets (must be the same in all datasets)
classes = ["pipeline"]  # change if you have multiple classes
merged_dataset_path="/content/merged_dataset"
data_yaml = {
    "path": "/content/merged_dataset",
    "train": "train/images",
    "val": "valid/images",
    "test": "test/images",
    "names": {i: name for i, name in enumerate(classes)}
}

with open(os.path.join(merged_dataset_path, "data.yaml"), "w") as f:
    yaml.dump(data_yaml, f)

print("✅ data.yaml created at", os.path.join(merged_dataset_path, "data.yaml"))

model = YOLO("yolov8n.pt")  # use yolov8s.pt, yolov8m.pt, etc. for bigger models

model.train(
    data=os.path.join("/content/merged_dataset", "data.yaml"),
    epochs=50,
    imgsz=640,
    batch=8
)

# Save best model path
best_model_path = model.ckpt_path
print("✅ Training complete. Best model saved at:", best_model_path)

from google.colab import drive

drive.mount('/content/drive')
model1 = "/content/runs/detect/train2/weights/best.pt"
drive_destination = "/content/drive/MyDrive/Colab Notebooks/Models/yolov8n_final.pt"

drive_destination = "/content/drive/MyDrive/Colab Notebooks/Models/yolov8n_final.pt"
model1 = "/content/runs/detect/train2/weights/best.pt"

# Copy best model to Drive
shutil.copy(model1, drive_destination)
print(f"📂 Model uploaded to Google Drive at: {drive_destination}")

# Path to model in Google Drive
#drive_model_path = "/content/drive/MyDrive/Colab Notebooks/Models/yolov8n.pt"

# Destination path in Colab
#colab_model_path = "/content/yolov8.pt"

# Copy model from Drive to Colab
#shutil.copy(drive_model_path, colab_model_path)
#print(f"📂 Model copied to Colab at: {colab_model_path}")

from google.colab import files
uploaded = files.upload()

video_path = list(uploaded.keys())[0]

"""## SENT

"""

import cv2
import numpy as np
np.float = np.float64  # Patch for ByteTrack

from ultralytics import YOLO
from yolox.tracker.byte_tracker import BYTETracker
from collections import defaultdict

class Args:
    track_thresh = 0.3
    track_buffer = 50
    match_thresh = 0.7
    frame_rate = 30
    mot20 = False

args = Args()

best_model_path = model1
model = YOLO(best_model_path)

video_path = "/content/WhatsApp Vidéo 2025-07-18 à 16.23.28_d90beaab.mp4"
cap = cv2.VideoCapture(video_path)
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = cap.get(cv2.CAP_PROP_FPS)
args.frame_rate = fps if fps > 0 else args.frame_rate

tracker = BYTETracker(args)
fourcc = cv2.VideoWriter_fourcc(*"mp4v")
out = cv2.VideoWriter("/content/tracking.mp4", fourcc, fps, (width, height))

class_colors = {
    "crack": (0, 0, 255),
    "pipeline": (255, 0, 0),
    "leak": (0, 255, 0),
    "puddle": (0, 255, 255)
}

class_names = model.names
track_trajectories = defaultdict(lambda: {'points': [], 'class_id': None})
pipeline_path_points = []

average_lines_per_frame = []
previous_x = None
previous_start_y = None
previous_end_y = None
previous_slope = 0

while True:
    ret, frame = cap.read()
    if not ret:
        break

    results = model(frame)[0]

    dets = []
    class_ids = []
    for box in results.boxes:
        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
        score = box.conf[0].item()
        cls_id = int(box.cls[0].item())
        dets.append([x1, y1, x2, y2, score])
        class_ids.append(cls_id)

    dets = np.array(dets) if len(dets) > 0 else np.empty((0, 5))
    online_targets = tracker.update(dets, (width, height), (width, height))

    frame_mid_lines = []

    for t in online_targets:
        track_id = t.track_id
        x, y, w, h = t.tlwh
        x1, y1, x2, y2 = int(x), int(y), int(x + w), int(y + h)

        best_iou = 0
        best_cls_id = -1
        for i, det in enumerate(dets):
            det_box = det[:4]
            track_box = [x, y, x + w, y + h]
            xi1 = max(det_box[0], track_box[0])
            yi1 = max(det_box[1], track_box[1])
            xi2 = min(det_box[2], track_box[2])
            yi2 = min(det_box[3], track_box[3])
            inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)
            det_area = (det_box[2] - det_box[0]) * (det_box[3] - det_box[1])
            track_area = w * h
            iou = inter_area / (det_area + track_area - inter_area)

            if iou > best_iou:
                best_iou = iou
                best_cls_id = class_ids[i]

        if best_cls_id != -1:
            class_name = class_names[best_cls_id]
            color = class_colors.get(class_name, (255, 255, 255))

            if track_trajectories[track_id]['class_id'] is None:
                track_trajectories[track_id]['class_id'] = best_cls_id

            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 3)
            cv2.putText(frame, f"ID: {track_id} ({class_name})", (x1, y1 - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)

            midline_color = (0, 255, 255)
            if (x2 - x1) > (y2 - y1):
                mid_y = (y1 + y2) // 2
                start_point = (x1, mid_y)
                end_point = (x2, mid_y)
            else:
                mid_x = (x1 + x2) // 2
                start_point = (mid_x, y1)
                end_point = (mid_x, y2)

            cv2.line(frame, start_point, end_point, midline_color, 2)
            frame_mid_lines.append((start_point, end_point))

            center = (int(x + w / 2), int(y + h / 2))
            track_trajectories[track_id]['points'].append(center)

            if class_name == "pipeline":
                pipeline_path_points.append(center)

    # Compute average middle line
    if frame_mid_lines:
        avg_start = (
            int(sum(pt[0][0] for pt in frame_mid_lines) / len(frame_mid_lines)),
            int(sum(pt[0][1] for pt in frame_mid_lines) / len(frame_mid_lines))
        )
        avg_end = (
            int(sum(pt[1][0] for pt in frame_mid_lines) / len(frame_mid_lines)),
            int(sum(pt[1][1] for pt in frame_mid_lines) / len(frame_mid_lines))
        )

        # Calculate slope & direction
        if previous_x is not None:
            delta_x = avg_start[0] - previous_x
            previous_slope = delta_x / (avg_end[1] - avg_start[1]) if (avg_end[1] - avg_start[1]) != 0 else 0
            if delta_x > 0:
                direction = "Right"
            elif delta_x < 0:
                direction = "Left"
            else:
                direction = "Straight"
        else:
            direction = "Unknown"
            previous_slope = 0

        previous_x = avg_start[0]
        previous_start_y = avg_start[1]
        previous_end_y = avg_end[1]

        cv2.line(frame, avg_start, avg_end, (255, 255, 255), 3)
        average_lines_per_frame.append((avg_start, avg_end, direction, previous_slope))
        print(f"Frame {int(cap.get(cv2.CAP_PROP_POS_FRAMES))}: Start {avg_start}, End {avg_end}, "
              f"Slope {previous_slope:.3f}, Direction {direction}")

    else:
        # Predict next line if YOLO fails
        if previous_x is not None:
            predicted_x = int(previous_x + previous_slope * (previous_end_y - previous_start_y))
            predicted_start = (predicted_x, previous_start_y)
            predicted_end = (predicted_x, previous_end_y)
            cv2.line(frame, predicted_start, predicted_end, (200, 200, 255), 3)  # light blue
            average_lines_per_frame.append((predicted_start, predicted_end, direction, previous_slope))
            print(f"Frame {int(cap.get(cv2.CAP_PROP_POS_FRAMES))}: PREDICTED Start {predicted_start}, End {predicted_end}, "
                  f"Slope {previous_slope:.3f}, Direction {direction}")
            previous_x = predicted_x  # update for next prediction

    out.write(frame)

cap.release()
out.release()

print("✅ Tracking complete. Output saved at /content/tracking.mp4")

# Save average line coordinates including slope & direction
txt_path = "/content/average_pipeline_path.txt"
with open(txt_path, "w") as f:
    for idx, (start, end, direction, slope) in enumerate(average_lines_per_frame, 1):
        f.write(f"Frame {idx}: Start {start}, End {end}, Direction {direction}, Slope {slope:.3f}\n")

print(f"Average pipeline path saved to {txt_path}")

#!/usr/bin/env python3
import cv2
import numpy as np
import torch
from ultralytics import YOLO
from yolox.tracker.byte_tracker import BYTETracker
from collections import defaultdict

np.float = np.float64  # Patch for ByteTrack

class Args:
    track_thresh = 0.3
    track_buffer = 50
    match_thresh = 0.7
    frame_rate = 30
    mot20 = False

args = Args()

best_model_path = "/content/runs/detect/train2/weights/best.pt"
model = YOLO(best_model_path)

video_path = "/content/WhatsApp Vidéo 2025-07-18 à 16.23.28_d90beaab.mp4"
output_video_path = "tracking.mp4"
output_txt_path = "average_pipeline_path1.txt"

cap = cv2.VideoCapture(video_path)
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = cap.get(cv2.CAP_PROP_FPS)
args.frame_rate = fps if fps > 0 else args.frame_rate

fourcc = cv2.VideoWriter_fourcc(*"mp4v")
out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))

tracker = BYTETracker(args)

class_colors = {
    "crack": (0, 0, 255),
    "pipeline": (255, 0, 0),
    "leak": (0, 255, 0),
    "puddle": (0, 255, 255)
}

class_names = model.names
track_trajectories = defaultdict(lambda: {'points': [], 'class_id': None})
pipeline_path_points = []

average_lines_per_frame = []
previous_x = None
previous_start_y = None
previous_end_y = None
previous_slope = 0

while True:
    ret, frame = cap.read()
    if not ret:
        break

    results = model(frame)[0]

    dets = []
    class_ids = []
    for box in results.boxes:
        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
        score = box.conf[0].item()
        cls_id = int(box.cls[0].item())
        dets.append([x1, y1, x2, y2, score])
        class_ids.append(cls_id)

    if len(dets) > 0:
        dets_tensor = torch.from_numpy(np.array(dets)).float()
    else:
        dets_tensor = torch.empty((0, 5))

    online_targets = tracker.update(dets_tensor, (width, height), (width, height))
    frame_mid_lines = []

    for t in online_targets:
        track_id = t.track_id
        x, y, w, h = t.tlwh
        x1, y1, x2, y2 = int(x), int(y), int(x + w), int(y + h)

        best_iou = 0
        best_cls_id = -1
        for i, det in enumerate(dets):
            det_box = det[:4]
            track_box = [x, y, x + w, y + h]
            xi1 = max(det_box[0], track_box[0])
            yi1 = max(det_box[1], track_box[1])
            xi2 = min(det_box[2], track_box[2])
            yi2 = min(det_box[3], track_box[3])
            inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)
            det_area = (det_box[2] - det_box[0]) * (det_box[3] - det_box[1])
            track_area = w * h
            iou = inter_area / (det_area + track_area - inter_area)
            if iou > best_iou:
                best_iou = iou
                best_cls_id = class_ids[i]

        if best_cls_id != -1:
            class_name = class_names[best_cls_id]
            color = class_colors.get(class_name, (255, 255, 255))
            if track_trajectories[track_id]['class_id'] is None:
                track_trajectories[track_id]['class_id'] = best_cls_id

            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 3)
            cv2.putText(frame, f"ID: {track_id} ({class_name})", (x1, y1 - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)

            if (x2 - x1) > (y2 - y1):
                mid_y = (y1 + y2) // 2
                start_point = (x1, mid_y)
                end_point = (x2, mid_y)
            else:
                mid_x = (x1 + x2) // 2
                start_point = (mid_x, y1)
                end_point = (mid_x, y2)

            cv2.line(frame, start_point, end_point, (0, 255, 255), 2)
            frame_mid_lines.append((start_point, end_point))

            center = (int(x + w / 2), int(y + h / 2))
            track_trajectories[track_id]['points'].append(center)

            if class_name == "pipeline":
                pipeline_path_points.append(center)

    if frame_mid_lines:
        avg_start = (
            int(sum(pt[0][0] for pt in frame_mid_lines) / len(frame_mid_lines)),
            int(sum(pt[0][1] for pt in frame_mid_lines) / len(frame_mid_lines))
        )
        avg_end = (
            int(sum(pt[1][0] for pt in frame_mid_lines) / len(frame_mid_lines)),
            int(sum(pt[1][1] for pt in frame_mid_lines) / len(frame_mid_lines))
        )

        if previous_x is not None:
            delta_x = avg_start[0] - previous_x
            previous_slope = delta_x / (avg_end[1] - avg_start[1]) if (avg_end[1] - avg_start[1]) != 0 else 0
            if delta_x > 0:
                direction = "Right"
            elif delta_x < 0:
                direction = "Left"
            else:
                direction = "Straight"
        else:
            direction = "Unknown"
            previous_slope = 0

        previous_x = avg_start[0]
        previous_start_y = avg_start[1]
        previous_end_y = avg_end[1]

        cv2.line(frame, avg_start, avg_end, (255, 255, 255), 3)
        average_lines_per_frame.append((avg_start, avg_end, direction, previous_slope))
        print(f"Frame {int(cap.get(cv2.CAP_PROP_POS_FRAMES))}: Slope {previous_slope:.3f}, Direction {direction}")

    else:
        if previous_x is not None:
            predicted_x = int(previous_x + previous_slope * (previous_end_y - previous_start_y))
            predicted_start = (predicted_x, previous_start_y)
            predicted_end = (predicted_x, previous_end_y)
            cv2.line(frame, predicted_start, predicted_end, (200, 200, 255), 3)
            average_lines_per_frame.append((predicted_start, predicted_end, direction, previous_slope))
            print(f"Frame {int(cap.get(cv2.CAP_PROP_POS_FRAMES))}: PREDICTED Slope {previous_slope:.3f}, Direction {direction}")
            previous_x = predicted_x

    out.write(frame)

cap.release()
out.release()

# Save average line coordinates
with open(output_txt_path, "w") as f:
    for idx, (start, end, direction, slope) in enumerate(average_lines_per_frame, 1):
        f.write(f"Frame {idx}: Start {start}, End {end}, Direction {direction}, Slope {slope:.3f}\n")

# Compute and print min/max slope
all_slopes = [slope for _, _, _, slope in average_lines_per_frame]
if all_slopes:
    print(f"Minimum slope: {min(all_slopes):.3f}")
    print(f"Maximum slope: {max(all_slopes):.3f}")

print("✅ Tracking complete. Output saved at", output_video_path)
print(f"Average pipeline path saved to {output_txt_path}")

#!/usr/bin/env python3
import cv2
import numpy as np
import torch
from ultralytics import YOLO
from yolox.tracker.byte_tracker import BYTETracker
from collections import defaultdict

np.float = np.float64  # Patch for ByteTrack

class Args:
    track_thresh = 0.3
    track_buffer = 50
    match_thresh = 0.7
    frame_rate = 30
    mot20 = False

args = Args()

best_model_path = "/content/runs/detect/train2/weights/best.pt"
model = YOLO(best_model_path)

video_path = "/content/WhatsApp Vidéo 2025-07-18 à 16.23.28_d90beaab.mp4"
output_video_path = "tracking.mp4"
output_txt_path = "average_pipeline_path_smooth.txt"

cap = cv2.VideoCapture(video_path)
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = cap.get(cv2.CAP_PROP_FPS)
args.frame_rate = fps if fps > 0 else args.frame_rate

fourcc = cv2.VideoWriter_fourcc(*"mp4v")
out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))

tracker = BYTETracker(args)

class_colors = {
    "crack": (0, 0, 255),
    "pipeline": (255, 0, 0),
    "leak": (0, 255, 0),
    "puddle": (0, 255, 255)
}

class_names = model.names
track_trajectories = defaultdict(lambda: {'points': [], 'class_id': None})
pipeline_path_points = []

average_lines_per_frame = []
previous_x = None
previous_start_y = None
previous_end_y = None
previous_slope = 0.0

# Parameters for smoothing and slope limit
alpha = 0.3              # smoothing factor (0 < alpha <= 1)
max_slope_change = 0.5    # maximum allowed slope change per frame

while True:
    ret, frame = cap.read()
    if not ret:
        break

    results = model(frame)[0]

    dets = []
    class_ids = []
    for box in results.boxes:
        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
        score = box.conf[0].item()
        cls_id = int(box.cls[0].item())
        dets.append([x1, y1, x2, y2, score])
        class_ids.append(cls_id)

    if len(dets) > 0:
        dets_tensor = torch.from_numpy(np.array(dets)).float()
    else:
        dets_tensor = torch.empty((0, 5))

    online_targets = tracker.update(dets_tensor, (width, height), (width, height))
    frame_mid_lines = []

    for t in online_targets:
        track_id = t.track_id
        x, y, w, h = t.tlwh
        x1, y1, x2, y2 = int(x), int(y), int(x + w), int(y + h)

        best_iou = 0
        best_cls_id = -1
        for i, det in enumerate(dets):
            det_box = det[:4]
            track_box = [x, y, x + w, y + h]
            xi1 = max(det_box[0], track_box[0])
            yi1 = max(det_box[1], track_box[1])
            xi2 = min(det_box[2], track_box[2])
            yi2 = min(det_box[3], track_box[3])
            inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)
            det_area = (det_box[2] - det_box[0]) * (det_box[3] - det_box[1])
            track_area = w * h
            iou = inter_area / (det_area + track_area - inter_area)
            if iou > best_iou:
                best_iou = iou
                best_cls_id = class_ids[i]

        if best_cls_id != -1:
            class_name = class_names[best_cls_id]
            color = class_colors.get(class_name, (255, 255, 255))
            if track_trajectories[track_id]['class_id'] is None:
                track_trajectories[track_id]['class_id'] = best_cls_id

            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 3)
            cv2.putText(frame, f"ID: {track_id} ({class_name})", (x1, y1 - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)

            if (x2 - x1) > (y2 - y1):
                mid_y = (y1 + y2) // 2
                start_point = (x1, mid_y)
                end_point = (x2, mid_y)
            else:
                mid_x = (x1 + x2) // 2
                start_point = (mid_x, y1)
                end_point = (mid_x, y2)

            cv2.line(frame, start_point, end_point, (0, 255, 255), 2)
            frame_mid_lines.append((start_point, end_point))

            center = (int(x + w / 2), int(y + h / 2))
            track_trajectories[track_id]['points'].append(center)

            if class_name == "pipeline":
                pipeline_path_points.append(center)

    if frame_mid_lines:
        avg_start = (
            int(sum(pt[0][0] for pt in frame_mid_lines) / len(frame_mid_lines)),
            int(sum(pt[0][1] for pt in frame_mid_lines) / len(frame_mid_lines))
        )
        avg_end = (
            int(sum(pt[1][0] for pt in frame_mid_lines) / len(frame_mid_lines)),
            int(sum(pt[1][1] for pt in frame_mid_lines) / len(frame_mid_lines))
        )

        if previous_x is not None:
            delta_x = avg_start[0] - previous_x
            raw_slope = delta_x / (avg_end[1] - avg_start[1]) if (avg_end[1] - avg_start[1]) != 0 else 0

            # Limit slope change
            slope_diff = raw_slope - previous_slope
            if abs(slope_diff) > max_slope_change:
                raw_slope = previous_slope + np.sign(slope_diff) * max_slope_change

            # Smooth slope
            previous_slope = alpha * raw_slope + (1 - alpha) * previous_slope

            if delta_x > 0:
                direction = "Right"
            elif delta_x < 0:
                direction = "Left"
            else:
                direction = "Straight"
        else:
            direction = "Unknown"
            previous_slope = 0.0

        previous_x = avg_start[0]
        previous_start_y = avg_start[1]
        previous_end_y = avg_end[1]

        cv2.line(frame, avg_start, avg_end, (255, 255, 255), 3)
        average_lines_per_frame.append((avg_start, avg_end, direction, previous_slope))
        print(f"Frame {int(cap.get(cv2.CAP_PROP_POS_FRAMES))}: Slope {previous_slope:.3f}, Direction {direction}")

    else:
        if previous_x is not None:
            predicted_x = int(previous_x + previous_slope * (previous_end_y - previous_start_y))
            predicted_start = (predicted_x, previous_start_y)
            predicted_end = (predicted_x, previous_end_y)
            cv2.line(frame, predicted_start, predicted_end, (200, 200, 255), 3)
            average_lines_per_frame.append((predicted_start, predicted_end, direction, previous_slope))
            print(f"Frame {int(cap.get(cv2.CAP_PROP_POS_FRAMES))}: PREDICTED Slope {previous_slope:.3f}, Direction {direction}")
            previous_x = predicted_x

    out.write(frame)

cap.release()
out.release()

with open(output_txt_path, "w") as f:
    for idx, (start, end, direction, slope) in enumerate(average_lines_per_frame, 1):
        f.write(f"Frame {idx}: Start {start}, End {end}, Direction {direction}, Slope {slope:.3f}\n")

# Print min/max slope
all_slopes = [slope for _, _, _, slope in average_lines_per_frame]
if all_slopes:
    print(f"Minimum slope: {min(all_slopes):.3f}")
    print(f"Maximum slope: {max(all_slopes):.3f}")

print("✅ Tracking complete. Output saved at", output_video_path)
print(f"Average pipeline path saved to {output_txt_path}")

"""## code + angle
The angle in this script is the angle of deviation of the pipeline relative to the horizontal axis in the image frame.
"""

#!/usr/bin/env python3
import cv2
import numpy as np
import torch
from ultralytics import YOLO
from yolox.tracker.byte_tracker import BYTETracker
from collections import defaultdict
import math

np.float = np.float64  # Patch for ByteTrack

class Args:
    track_thresh = 0.3
    track_buffer = 50
    match_thresh = 0.7
    frame_rate = 30
    mot20 = False

args = Args()

best_model_path = drive_destination
model = YOLO(best_model_path)

video_path = "/content/WhatsApp Vidéo 2025-07-18 à 16.23.28_d90beaab.mp4"
output_video_path = "tracking.mp4"
output_txt_path = "average_pipeline_path_angles.txt"

cap = cv2.VideoCapture(video_path)
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = cap.get(cv2.CAP_PROP_FPS)
args.frame_rate = fps if fps > 0 else args.frame_rate

fourcc = cv2.VideoWriter_fourcc(*"mp4v")
out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))

tracker = BYTETracker(args)

class_colors = {
    "crack": (0, 0, 255),
    "pipeline": (255, 0, 0),
    "leak": (0, 255, 0),
    "puddle": (0, 255, 255)
}

class_names = model.names
track_trajectories = defaultdict(lambda: {'points': [], 'class_id': None})
pipeline_path_points = []

average_lines_per_frame = []
previous_x = None
previous_start_y = None
previous_end_y = None
previous_angle = 0  # Store angle instead of slope

def slope_to_angle(slope):
    """Convert slope to angle in degrees"""
    angle_rad = math.atan(slope)
    angle_deg = math.degrees(angle_rad)
    return angle_deg

while True:
    ret, frame = cap.read()
    if not ret:
        break

    results = model(frame)[0]

    dets = []
    class_ids = []
    for box in results.boxes:
        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
        score = box.conf[0].item()
        cls_id = int(box.cls[0].item())
        dets.append([x1, y1, x2, y2, score])
        class_ids.append(cls_id)

    if len(dets) > 0:
        dets_tensor = torch.from_numpy(np.array(dets)).float()
    else:
        dets_tensor = torch.empty((0, 5))

    online_targets = tracker.update(dets_tensor, (width, height), (width, height))
    frame_mid_lines = []

    for t in online_targets:
        track_id = t.track_id
        x, y, w, h = t.tlwh
        x1, y1, x2, y2 = int(x), int(y), int(x + w), int(y + h)

        best_iou = 0
        best_cls_id = -1
        for i, det in enumerate(dets):
            det_box = det[:4]
            track_box = [x, y, x + w, y + h]
            xi1 = max(det_box[0], track_box[0])
            yi1 = max(det_box[1], track_box[1])
            xi2 = min(det_box[2], track_box[2])
            yi2 = min(det_box[3], track_box[3])
            inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)
            det_area = (det_box[2] - det_box[0]) * (det_box[3] - det_box[1])
            track_area = w * h
            iou = inter_area / (det_area + track_area - inter_area)
            if iou > best_iou:
                best_iou = iou
                best_cls_id = class_ids[i]

        if best_cls_id != -1:
            class_name = class_names[best_cls_id]
            color = class_colors.get(class_name, (255, 255, 255))
            if track_trajectories[track_id]['class_id'] is None:
                track_trajectories[track_id]['class_id'] = best_cls_id

            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 3)
            cv2.putText(frame, f"ID: {track_id} ({class_name})", (x1, y1 - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)

            if (x2 - x1) > (y2 - y1):
                mid_y = (y1 + y2) // 2
                start_point = (x1, mid_y)
                end_point = (x2, mid_y)
            else:
                mid_x = (x1 + x2) // 2
                start_point = (mid_x, y1)
                end_point = (mid_x, y2)

            cv2.line(frame, start_point, end_point, (0, 255, 255), 2)
            frame_mid_lines.append((start_point, end_point))

            center = (int(x + w / 2), int(y + h / 2))
            track_trajectories[track_id]['points'].append(center)

            if class_name == "pipeline":
                pipeline_path_points.append(center)

    if frame_mid_lines:
        avg_start = (
            int(sum(pt[0][0] for pt in frame_mid_lines) / len(frame_mid_lines)),
            int(sum(pt[0][1] for pt in frame_mid_lines) / len(frame_mid_lines))
        )
        avg_end = (
            int(sum(pt[1][0] for pt in frame_mid_lines) / len(frame_mid_lines)),
            int(sum(pt[1][1] for pt in frame_mid_lines) / len(frame_mid_lines))
        )

        if previous_x is not None:
            delta_x = avg_start[0] - previous_x
            slope = delta_x / (avg_end[1] - avg_start[1]) if (avg_end[1] - avg_start[1]) != 0 else 0
            angle = slope_to_angle(slope)
            previous_angle = angle

            if delta_x > 0:
                direction = "Right"
            elif delta_x < 0:
                direction = "Left"
            else:
                direction = "Straight"
        else:
            direction = "Unknown"
            previous_angle = 0

        previous_x = avg_start[0]
        previous_start_y = avg_start[1]
        previous_end_y = avg_end[1]

        cv2.line(frame, avg_start, avg_end, (255, 255, 255), 3)
        average_lines_per_frame.append((avg_start, avg_end, direction, previous_angle))
        print(f"Frame {int(cap.get(cv2.CAP_PROP_POS_FRAMES))}: Angle {previous_angle:.2f}°, Direction {direction}")

    else:
        if previous_x is not None:
            predicted_x = int(previous_x + math.tan(math.radians(previous_angle)) * (previous_end_y - previous_start_y))
            predicted_start = (predicted_x, previous_start_y)
            predicted_end = (predicted_x, previous_end_y)
            cv2.line(frame, predicted_start, predicted_end, (200, 200, 255), 3)
            average_lines_per_frame.append((predicted_start, predicted_end, direction, previous_angle))
            print(f"Frame {int(cap.get(cv2.CAP_PROP_POS_FRAMES))}: PREDICTED Angle {previous_angle:.2f}°, Direction {direction}")
            previous_x = predicted_x

    out.write(frame)

cap.release()
out.release()

# Save average line coordinates
with open(output_txt_path, "w") as f:
    for idx, (start, end, direction, angle) in enumerate(average_lines_per_frame, 1):
        f.write(f"Frame {idx}: Start {start}, End {end}, Direction {direction}, Angle {angle:.2f}°\n")

# Compute and print min/max angle
all_angles = [angle for _, _, _, angle in average_lines_per_frame]
if all_angles:
    print(f"Minimum angle: {min(all_angles):.2f}°")
    print(f"Maximum angle: {max(all_angles):.2f}°")

print("✅ Tracking complete. Output saved at", output_video_path)
print(f"Average pipeline path saved to {output_txt_path}")

"""###code angle with horizontal changes of angles"""

#!/usr/bin/env python3
import cv2
import numpy as np
import torch
from ultralytics import YOLO
from yolox.tracker.byte_tracker import BYTETracker
from collections import defaultdict
import math

np.float = np.float64  # Patch for ByteTrack

class Args:
    track_thresh = 0.3
    track_buffer = 50
    match_thresh = 0.7
    frame_rate = 30
    mot20 = False

args = Args()


best_model_path = drive_destination
model = YOLO(best_model_path)

video_path = "/content/WhatsApp Vidéo 2025-07-18 à 16.04.22_f1bd593e.mp4"
output_video_path = "tracking_final.mp4"
output_txt_path = "pipeline_full_data.txt"

cap = cv2.VideoCapture(video_path)
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = cap.get(cv2.CAP_PROP_FPS)
args.frame_rate = fps if fps > 0 else args.frame_rate

fourcc = cv2.VideoWriter_fourcc(*"mp4v")
out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))

tracker = BYTETracker(args)

class_colors = {
    "crack": (0, 0, 255),
    "pipeline": (255, 0, 0),
    "leak": (0, 255, 0),
    "puddle": (0, 255, 255)
}

class_names = model.names
track_trajectories = defaultdict(lambda: {'points': [], 'class_id': None})
pipeline_path_points = []

previous_avg_start = None
previous_avg_end = None

frame_data = []

while True:
    ret, frame = cap.read()
    if not ret:
        break

    results = model(frame)[0]

    dets = []
    class_ids = []
    for box in results.boxes:
        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
        score = box.conf[0].item()
        cls_id = int(box.cls[0].item())
        dets.append([x1, y1, x2, y2, score])
        class_ids.append(cls_id)

    if len(dets) > 0:
        dets_tensor = torch.from_numpy(np.array(dets)).float()
    else:
        dets_tensor = torch.empty((0, 5))

    online_targets = tracker.update(dets_tensor, (width, height), (width, height))
    frame_mid_lines = []

    for t in online_targets:
        track_id = t.track_id
        x, y, w, h = t.tlwh
        x1, y1, x2, y2 = int(x), int(y), int(x + w), int(y + h)

        best_iou = 0
        best_cls_id = -1
        for i, det in enumerate(dets):
            det_box = det[:4]
            track_box = [x, y, x + w, y + h]
            xi1 = max(det_box[0], track_box[0])
            yi1 = max(det_box[1], track_box[1])
            xi2 = min(det_box[2], track_box[2])
            yi2 = min(det_box[3], track_box[3])
            inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)
            det_area = (det_box[2] - det_box[0]) * (det_box[3] - det_box[1])
            track_area = w * h
            iou = inter_area / (det_area + track_area - inter_area)
            if iou > best_iou:
                best_iou = iou
                best_cls_id = class_ids[i]

            if best_cls_id != -1:
                class_name = class_names[best_cls_id]
                color = class_colors.get(class_name, (255, 255, 255))

                # ✅ Draw bounding box
                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 3)
                cv2.putText(frame, f"ID: {track_id} ({class_name})", (x1, y1 - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)

            # ✅ Draw mid-line for the detection
            if (x2 - x1) > (y2 - y1):
                mid_y = (y1 + y2) // 2
                start_point = (x1, mid_y)
                end_point = (x2, mid_y)
            else:
                mid_x = (x1 + x2) // 2
                start_point = (mid_x, y1)
                end_point = (mid_x, y2)

            cv2.line(frame, start_point, end_point, (255, 255, 255), 2)
            frame_mid_lines.append((start_point, end_point))


    if frame_mid_lines:
        avg_start = (
            int(sum(pt[0][0] for pt in frame_mid_lines) / len(frame_mid_lines)),
            int(sum(pt[0][1] for pt in frame_mid_lines) / len(frame_mid_lines))
        )
        avg_end = (
            int(sum(pt[1][0] for pt in frame_mid_lines) / len(frame_mid_lines)),
            int(sum(pt[1][1] for pt in frame_mid_lines) / len(frame_mid_lines))
        )

        if previous_avg_start is not None and previous_avg_end is not None:
            delta_x = avg_start[0] - previous_avg_start[0]
            delta_y = avg_end[1] - previous_avg_end[1]
            angle_deg = math.degrees(math.atan2(delta_y, delta_x)) if delta_x != 0 else 90.0 * (1 if delta_y >= 0 else -1)
        else:
            delta_x = 0
            delta_y = 0
            angle_deg = 0

        previous_avg_start = avg_start
        previous_avg_end = avg_end

        cv2.line(frame, avg_start, avg_end, (255, 255, 255), 3)
        frame_data.append((avg_start, avg_end, delta_x, delta_y, angle_deg))
        print(f"Frame {int(cap.get(cv2.CAP_PROP_POS_FRAMES))}: Start={avg_start}, End={avg_end}, Δx={delta_x}, Δy={delta_y}, Angle={angle_deg:.2f}°")

    out.write(frame)

cap.release()
out.release()

# Save full line coordinates + deviation + angles
with open(output_txt_path, "w") as f:
    for idx, (start, end, dx, dy, angle) in enumerate(frame_data, 1):
        f.write(f"Frame {idx}: Start={start}, End={end}, Δx={dx}, Δy={dy}, Angle={angle:.2f}°\n")

print("✅ Tracking complete. Output saved at", output_video_path)
print(f"Full pipeline line coordinates + deviation saved to {output_txt_path}")

